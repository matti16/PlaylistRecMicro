{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <hr style=\"clear: both\" />\n",
    "\n",
    "# Running Spark in YARN-client mode\n",
    "\n",
    "This notebook demonstrates how to set up a SparkContext that uses SURFsara's Hadoop cluster: [YARN resourcemanager](http://head05.hathi.surfsara.nl:8088/cluster) (note you will need to be authenticated via kerberos on your machine to visit the resourcemanager link) for executors.\n",
    "\n",
    "First initialize kerberos via a Jupyter terminal. \n",
    "In the terminal execute: <BR>\n",
    "<i>kinit -k -t data/robertop.keytab robertop@CUA.SURFSARA.NL</i><BR>\n",
    "Print your credentials:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticket cache: FILE:/tmp/krb5cc_1000\r\n",
      "Default principal: robertop@CUA.SURFSARA.NL\r\n",
      "\r\n",
      "Valid starting       Expires              Service principal\r\n",
      "04/26/2016 06:57:12  04/27/2016 06:57:12  krbtgt/CUA.SURFSARA.NL@CUA.SURFSARA.NL\r\n",
      "\trenew until 04/26/2016 06:57:12\r\n"
     ]
    }
   ],
   "source": [
    "! klist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\r\n",
      "drwx------   - robertop hdfs          0 2016-04-26 14:23 .Trash\r\n",
      "drwxr-xr-x   - robertop hdfs          0 2016-04-26 14:23 .sparkStaging\r\n",
      "drwx------   - robertop hdfs          0 2016-04-06 15:54 .staging\r\n",
      "drwxr-xr-x   - robertop hdfs          0 2016-04-26 14:23 mattia\r\n",
      "drwxr-xr-x   - robertop hdfs          0 2016-04-13 10:00 recsys2016Competition\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls \n",
    "execfile('../spark-scripts/bullet.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that we can browse HDFS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next initialize Spark. Note that the code below starts a job on the Hadoop cluster that will remain running while the notebook is active. Please close and halt the notebook when you are done. Starting the SparkContext can take a little longer. You can check the YARN resourcemanager to see the current status/usage of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/local/bin/python2.7'\n",
    "\n",
    "HDFS_PATH = \"hdfs://hathi-surfsara\"\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "sconf = SparkConf()\n",
    "\n",
    "# Master is now yarn-client. The YARN and hadoop config is read from the environment\n",
    "sconf.setMaster(\"yarn-client\")\n",
    "\n",
    "# You can control many Spark settings via the SparkConf. This determines the amount of executors on the cluster:\n",
    "sconf.set(\"spark.executor.instances\", \"200\")\n",
    "#sconf.set(\"spark.executor.memory\", \"20g\")\n",
    "\n",
    "# UFW (firewall) is active on the VM. We explicitly opened these ports and Spark should not bind to random ports:\n",
    "sconf.set(\"spark.driver.port\", 51800)\n",
    "sconf.set(\"spark.fileserver.port\", 51801)\n",
    "sconf.set(\"spark.broadcast.port\", 51802)\n",
    "sconf.set(\"spark.replClassServer.port\", 51803)\n",
    "sconf.set(\"spark.blockManager.port\", 51804)\n",
    "sconf.set(\"spark.authenticate\", True)\n",
    "sconf.set(\"spark.yarn.keytab\", \"/home/jovyan/work/data/robertop.keytab\")\n",
    "sconf.set(\"spark.yarn.access.namenodes\", HDFS_PATH + \":8020\")\n",
    "\n",
    "try:\n",
    "    sc = SparkContext(conf=sconf)\n",
    "    sqlCtx = SQLContext(sc) \n",
    "    sendNotificationToMattia(\"Spark Context\", \"Ready!\")\n",
    "except Exception, err:\n",
    "    sendNotificationToMattia(\"Fuck you!\", str(err)) \n",
    "    print str(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <hr style=\"clear: both\" />\n",
    "\n",
    "# Now you can run your code\n",
    "\n",
    "Pick a clustering algorithm (name of the file that provides a classify(x,y [,threshold]) function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "751\n"
     ]
    }
   ],
   "source": [
    "execfile('../spark-scripts/conventions.py')\n",
    "execfile('../spark-scripts/splitCluster.py')\n",
    "#execfile('../spark-scripts/utils.py')\n",
    "execfile('../spark-scripts/eval.py')\n",
    "execfile('../spark-scripts/implicitPlaylistAlgoFunctions.py')\n",
    "execfile('../spark-scripts/implicitPlaylistAlgoMain.py')\n",
    "\n",
    "CLUSTER_ALGO = 'jaccardBase'\n",
    "THRESHOLD = 0.751\n",
    "print str(THRESHOLD)[2:]\n",
    "execfile('../spark-scripts/' + CLUSTER_ALGO + '.py')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the conf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "\n",
    "BASE_PATH = HDFS_PATH + '/user/robertop/mattia'\n",
    "\n",
    "conf = {}\n",
    "\n",
    "conf['split'] = {}\n",
    "conf['split']['reclistSize'] = 100\n",
    "conf['split']['callParams'] = {}\n",
    "conf['split']['excludeAlreadyListenedTest'] = True\n",
    "conf['split']['name'] = 'test'\n",
    "conf['split']['split'] = conf['split']['name']\n",
    "conf['split']['minEventsPerUser'] = 5\n",
    "conf['split']['inputData'] = HDFS_PATH + '/user/robertop/mattia/clusterBase.split/SenzaRipetizioni_1'\n",
    "#conf['split']['inputData'] = 's3n://contentwise-research-poli/30musicdataset/newFormat/relations/sessions.idomaar'\n",
    "conf['split']['bucketName'] = BASE_PATH\n",
    "conf['split']['percUsTr'] = 0.05\n",
    "conf['split']['ts'] = int(0.75 * (1421745857 - 1390209860) + 1390209860) - 10000\n",
    "conf['split']['minEventPerSession'] = 5\n",
    "conf['split']['onlineTrainingLength'] = 5\n",
    "conf['split']['GTlength'] = 1\n",
    "conf['split']['minEventPerSessionTraining'] = 10\n",
    "conf['split']['minEventPerSessionTest'] = 11\n",
    "conf['split']['mode'] = 'session'\n",
    "conf['split']['forceSplitCreation'] = False\n",
    "conf['split'][\"prop\"] = {'reclistSize': conf['split']['reclistSize']}\n",
    "conf['split']['type'] = list\n",
    "conf['split']['out'] = HDFS_PATH + '/user/robertop/mattia/clusterBase.split/'\n",
    "conf['split']['location'] = '30Mdataset/relations/sessions'\n",
    "\n",
    "conf['evaluation'] = {}\n",
    "conf['evaluation']['metric'] = {}\n",
    "conf['evaluation']['metric']['type'] = 'recall'\n",
    "conf['evaluation']['metric']['prop'] = {}\n",
    "conf['evaluation']['metric']['prop']['N'] = [1,2,5,10,15,20,25,50,100]\n",
    "conf['evaluation']['name'] = 'recall@N'\n",
    "\n",
    "conf['general'] = {}\n",
    "conf['general']['clientname'] = \"clusterBase.split\"\n",
    "conf['general']['bucketName'] = 'head02.hathi.surfsara.nl/user/robertop/mattia'\n",
    "conf['general']['tracksPath'] = '30Mdataset/entities/tracks.idomaar.gz'\n",
    "\n",
    "conf['algo'] = {}\n",
    "conf['algo']['name'] = 'ClusterBase'\n",
    "conf['algo']['props'] = {}\n",
    "# ***** EXAMPLE OF CONFIGURATION *****#\n",
    "conf['algo']['props'][\"sessionJaccardShrinkage\"] = 5\n",
    "conf['algo']['props'][\"clusterSimilarityThreshold\"] = 0.1\n",
    "conf['algo']['props'][\"expDecayFactor\"] = 0.7\n",
    "# ****** END EXAMPLE ****************#\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Load the list of songs ad create clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracks: 4519105\n",
      "IDs: 4519105\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import string\n",
    "\n",
    "def my_replace_punct(x):\n",
    "    ret = \"\"\n",
    "    for i in x:\n",
    "        if i == '+':\n",
    "            ret += ' '\n",
    "        else:\n",
    "            ret += i\n",
    "    return ret\n",
    "\n",
    "tracksRDD = sc.textFile(BASE_PATH + '/30Mdataset/entities/tracks.idomaar.gz')\n",
    "tracksRDD = tracksRDD.repartition(200)\n",
    "tracksRDD = tracksRDD.map(lambda x: x.split('\\t')).map(lambda x: (x[1], json.loads(x[3])['name'].split('/') ) )\n",
    "tracksRDD = tracksRDD.map(lambda x: (x[0], \" \".join( (x[1][0], x[1][2]) ) )).distinct()\n",
    "tracksRDD = tracksRDD.map(lambda x : (x[0], my_replace_punct(x[1])))\n",
    "tracksRDD = tracksRDD.map(lambda x: (x[0], tokenize_song(x[1]), x[1]))\n",
    "\n",
    "#tracksRDD.saveAsPickleFile(BASE_PATH + '/30Mdataset/entities/tracksPickle')\n",
    "tracksIdsRDD = tracksRDD.map(lambda x: (int(x[0]), [x[0]]))\n",
    "\n",
    "try:\n",
    "    n_tracks = str(tracksRDD.count())\n",
    "    n_tracksID = str(tracksIdsRDD.count())\n",
    "    print \"Tracks: \" + n_tracks\n",
    "    print \"IDs: \" + n_tracksID\n",
    "    parts = str(tracksRDD.getNumPartitions())\n",
    "    sendNotificationToMattia(\"Tracks loaded. \" + parts + \" parts\", \"Tracks: \" + n_tracks + \"IDs: \" + n_tracksID)\n",
    "except Exception, err:\n",
    "    sendNotificationToMattia(\"Fuck you!\", str(err))    \n",
    "    print str(err)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove the 100 longest songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'74141', u'750321', u'3152622', u'2827538', u'3152620', u'3152621', u'2827545', u'3152619', u'3341180', u'3152623', u'3152618', u'2827540', u'2827541', u'2827552', u'2827542', u'141134', u'2827551', u'2827550', u'781026', u'2827544', u'2827553', u'1674589', u'3363685', u'2827546', u'2827549', u'2827543', u'2680576', u'3544157', u'2827547', u'2827548', u'3544156', u'2827539', u'2754946', u'928716', u'3152606', u'3375879', u'3152605', u'3152604', u'3152607', u'3152602', u'105580', u'3152603', u'3152601', u'3152608', u'1230495', u'410557', u'742648', u'2171839', u'2652403', u'2520106', u'3068573', u'3396589', u'1994217', u'3095948', u'2036942', u'2706385', u'1994226', u'3755457', u'3395838', u'1675513', u'1994216', u'922515', u'1994220', u'1886095', u'1600901', u'2955525', u'1994227', u'482609', u'1994221', u'1994214', u'1668615', u'1089324', u'2696994', u'2696995', u'828373', u'3206202', u'1089325', u'1994223', u'1807301', u'2702002', u'2696998', u'3341156', u'1994229', u'760742', u'751407', u'1886115', u'1994212', u'1994213', u'24041', u'1994228', u'3396574', u'1994218', u'1994219', u'1994222', u'2078372', u'2418300', u'1434551', u'3341167', u'3396988', u'1077883']\n"
     ]
    }
   ],
   "source": [
    "TOP_N = 100\n",
    "longSong = tracksRDD.map(lambda x: (x[0], len(x[1]))).takeOrdered(TOP_N, lambda x: -x[1])\n",
    "longSong = [i[0] for i in longSong]\n",
    "print longSong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4519005\n"
     ]
    }
   ],
   "source": [
    "REMOVE_LONG_TITLES = True\n",
    "if REMOVE_LONG_TITLES:\n",
    "    tracksRDD = tracksRDD.filter(lambda x: x[0] not in longSong)\n",
    "    print tracksRDD.count()\n",
    "    \n",
    "TESTING = False\n",
    "if TESTING:\n",
    "    tracksRDD = tracksRDD.take(50000)\n",
    "    tracksRDD = sc.parallelize(tracksRDD)\n",
    "    print tracksRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build RDD with word/bigram mapped to songs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'talk talk', (u'2740924', u'Talk Talk Such A Shame (Dub Mix)')), (u'talk such', (u'2740924', u'Talk Talk Such A Shame (Dub Mix)')), (u'such a', (u'2740924', u'Talk Talk Such A Shame (Dub Mix)'))]\n"
     ]
    }
   ],
   "source": [
    "#Map to Bigram -> (id,name)\n",
    "def extract_bigrams(x):\n",
    "    result = []\n",
    "    tokens = x[1]\n",
    "    l = len(tokens)\n",
    "    for i in range(l-1):\n",
    "        bigram = tokens[i] + ' ' + tokens[i+1]\n",
    "        result.append((bigram, (x[0], x[2])))\n",
    "    return result\n",
    "\n",
    "#Map to word -> (id,name)\n",
    "def extract_words(x):\n",
    "    result = []\n",
    "    tokens = x[1]\n",
    "    for t in tokens:\n",
    "        result.append( (t, (x[0], x[2])) )\n",
    "    return result\n",
    " \n",
    "''' CHOOSE ONE OF THE ABOVE FUNCTIONS FOR THE FLAT MAP '''\n",
    "#Build an RDD with ('word' or 'bigram' -> (id, name))\n",
    "wordsRDD = tracksRDD.flatMap( extract_bigrams )\n",
    "print wordsRDD.take(3)\n",
    "\n",
    "#Group by 'word' and keep only the ones with more then 1 song\n",
    "wordsRDD = wordsRDD.groupByKey().filter(lambda x: len(x[1]) > 1)\n",
    "\n",
    "#Eliminate words in top 0.1% of words\n",
    "try:\n",
    "    total_words = wordsRDD.count()\n",
    "    print \"\\nTotal words with more than one song: \" + str(total_words)\n",
    "    \n",
    "    nwords = 20 #int(total_words*0.001)\n",
    "    top_words_count = wordsRDD.map(lambda x: (x[0], len(x[1])))\n",
    "    \n",
    "    total_number_of_songs = top_words_count.reduce(lambda x,y: (0, x[1] + y[1]))[1]\n",
    "    print \"Total number of songs present: \" + total_number_of_songs\n",
    "    \n",
    "    top_words_count = top_words_count.takeOrdered(nwords, lambda x: -x[1])\n",
    "    \n",
    "    print str(top_words_count) + \"\\n\"\n",
    "    \n",
    "    song_removed = 0\n",
    "    top_words = []\n",
    "    for i in top_words_count:\n",
    "        song_removed += i[1]\n",
    "        top_words.append(i[0])\n",
    "    \n",
    "    print \"\\nSong removed: \" + str(song_removed)\n",
    "    \n",
    "    wordsRDD = wordsRDD.filter(lambda x: x[0] not in top_words)\n",
    "    \n",
    "    sendNotificationToMattia(\"Removed Words - Songs: removed \" + str(song_removed) + \" / \" + str(total_number_of_songs) , str(top_words_count))\n",
    "    \n",
    "except Exception, err:\n",
    "    sendNotificationToMattia(\"Fuck you!\", str(err))    \n",
    "    print str(err)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify songs inside the same list (word/bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Compute a cartesian product for each list of songs with a common word\n",
    "def filtered_cartesian(x):\n",
    "    equal_couples = set()\n",
    "    x_1 = list(x[1])\n",
    "    for i in range(len(x_1)):\n",
    "        a = x_1[i]\n",
    "        id_a = x_1[i][0]\n",
    "        name_a = x_1[i][1]\n",
    "        \n",
    "        for j in range(i):\n",
    "            b = x_1[j]\n",
    "            id_b = x_1[j][0]\n",
    "            name_b = x_1[j][1]\n",
    "            if id_a != id_b:\n",
    "                if classify(name_a, name_b, THRESHOLD):\n",
    "                    equal_couples.add( (int(id_a), (id_a, id_b)) )\n",
    "                    equal_couples.add( (int(id_b), (id_a, id_b)) )\n",
    "                    \n",
    "    return list(equal_couples)\n",
    "\n",
    "coupleRDD = wordsRDD.flatMap(filtered_cartesian)\n",
    "try:\n",
    "    n_couples = coupleRDD.count()\n",
    "    print n_couples\n",
    "    sendNotificationToMattia(\"Words!\", str(n_couples) + \" Couples found.\")\n",
    "except Exception, err:\n",
    "    sendNotificationToMattia(\"Fuck you!\", str(err))\n",
    "    print str(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group by song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Group by key (song). Each song has now one cluster\n",
    "def merge_couples(x, y):\n",
    "    return list(set(x) | set(y))\n",
    "\n",
    "try:\n",
    "    songClusterRDD = coupleRDD.reduceByKey(merge_couples)\n",
    "    print songClusterRDD.count()\n",
    "    print songClusterRDD.take(10)\n",
    "    sendNotificationToMattia(\"Song to Cluster!\", str(songClusterRDD.take(10)))\n",
    "except Exception, err:\n",
    "    sendNotificationToMattia(\"Fuck you!\", str(err))\n",
    "    print str(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify with all the Songs (not clustered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5675143\n",
      "[(4915200, [u'4915200']), (4915200, [u'4915200']), (2457600, [u'2457599', u'2457600']), (2457600, [u'2457599', u'2457600']), (1796000, [u'1796000']), (898000, [u'898000']), (0, [u'0']), (3050800, [u'3050800']), (4522000, [u'4522000']), (2152800, [u'2152800'])]\n"
     ]
    }
   ],
   "source": [
    "#In this way we obtain a complete RDD with song -> group of songs\n",
    "def reduce_to_biggest(x, y):\n",
    "    bigger = x if len(x) > len(y) else y\n",
    "    result = sorted(bigger)\n",
    "    return result\n",
    " \n",
    "unionJoinRDD = tracksIdsRDD.leftOuterJoin(songClusterRDD)\n",
    "unionRDD = unionJoinRDD.map(lambda x: (x[0], x[1][0]) if x[1][1] == None else (x[0], x[1][1]))\n",
    "\n",
    "try:\n",
    "    tot_n = unionRDD.count()\n",
    "    print tot_n\n",
    "    print unionRDD.take(10)\n",
    "    sendNotificationToMattia(str(tot_n) + \" Tracks unified with all tracks ids\", \"Inverting and unifying clusters...\")\n",
    "except Exception, err:\n",
    "    sendNotificationToMattia(\"Fuck you!\", str(err))\n",
    "    print str(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build indexing for clusters (new IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Flip the mapping as cluster->song\n",
    "clusterSongsRDD = unionRDD.map(lambda x: (' '.join(x[1]), x[0])).groupByKey().mapValues(list)\n",
    "clusterSongsRDD = clusterSongsRDD.zipWithIndex().map(lambda x: (x[1], x[0][1]))\n",
    "\n",
    "try:\n",
    "    tot_n = clusterSongsRDD.count()\n",
    "    print tot_n\n",
    "    sendNotificationToMattia(str(tot_n) + \" Unique clusters found!\", \"Writing...\")\n",
    "except Exception, err:\n",
    "    sendNotificationToMattia(\"Fuck you!\", str(err))\n",
    "    print str(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Save clustering\n",
    "THRESHOLD_STR = str(THRESHOLD)[2:]\n",
    "try:\n",
    "    clusterSongsRDD.saveAsPickleFile(BASE_PATH + '/clusters/' + CLUSTER_ALGO + THRESHOLD_STR)\n",
    "    sendNotificationToMattia(\"File Written!\", BASE_PATH + '/clusters/' + CLUSTER_ALGO + THRESHOLD_STR)\n",
    "except Exception, err:\n",
    "    sendNotificationToMattia(\"Fuck you!\", str(err))\n",
    "    print str(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(819200, ([u'819200'], None)),\n",
       " (2048000, ([u'2048000'], None)),\n",
       " (1796000, ([u'1796000'], None)),\n",
       " (0, ([u'0'], None)),\n",
       " (3050800, ([u'3050800'], None)),\n",
       " (3702800, ([u'3702800'], None)),\n",
       " (356800, ([u'356800'], None)),\n",
       " (2490400, ([u'2490400'], None)),\n",
       " (2490400, ([u'2490400'], None)),\n",
       " (551600, ([u'551600'], None)),\n",
       " (551600, ([u'551600'], None)),\n",
       " (713600, ([u'713600'], None)),\n",
       " (4662400, ([u'4662400'], None)),\n",
       " (2866400, ([u'2866400'], None)),\n",
       " (1968400, ([u'1968400'], None)),\n",
       " (1278000, ([u'1278000'], None)),\n",
       " (172400, ([u'172400'], None)),\n",
       " (5019200, ([u'5019200'], None)),\n",
       " (2325200, ([u'2325200'], None)),\n",
       " (2325200, ([u'2325200'], None)),\n",
       " (1427200, ([u'1427200'], None)),\n",
       " (1427200, ([u'1427200'], None)),\n",
       " (1427200, ([u'1427200'], None)),\n",
       " (529200, ([u'529200'], None)),\n",
       " (2523200, ([u'2523200'], None)),\n",
       " (2523200, ([u'2523200'], None)),\n",
       " (1922400, ([u'1922400'], None)),\n",
       " (4478000, ([u'4478000'], None)),\n",
       " (3580000, ([u'3580000'], None)),\n",
       " (2682000, ([u'2682000'], None)),\n",
       " (1310800, ([u'1310800'], None)),\n",
       " (4834800, ([u'4834800'], None)),\n",
       " (3038800, ([u'3038800'], [u'3038802', u'3038800'])),\n",
       " (2140800, ([u'2140800'], None)),\n",
       " (1242800, ([u'1242800'], None)),\n",
       " (2556000, ([u'2556000'], None)),\n",
       " (2556000, ([u'2556000'], None)),\n",
       " (835600, ([u'835600'], None)),\n",
       " (4820800, ([u'4820800'], None)),\n",
       " (2497600, ([u'2497600'], None)),\n",
       " (2497600, ([u'2497600'], None)),\n",
       " (1599600, ([u'1599600'], None)),\n",
       " (1599600, ([u'1599600'], None)),\n",
       " (701600, ([u'701600'], None)),\n",
       " (2982000, ([u'2982000'], None)),\n",
       " (3752400, ([u'3752400'], None)),\n",
       " (3707200, ([u'3707200'], None)),\n",
       " (2854400, ([u'2854400'], None)),\n",
       " (160400, ([u'160400'], None)),\n",
       " (2588800, ([u'2588800'], None))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unionJoinRDD = tracksIdsRDD.leftOuterJoin(songClusterRDD)\n",
    "unionJoinRDD.take(50)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4519105"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
