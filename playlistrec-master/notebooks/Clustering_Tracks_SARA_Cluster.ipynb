{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <hr style=\"clear: both\" />\n",
    "\n",
    "# Running Spark in YARN-client mode\n",
    "\n",
    "This notebook demonstrates how to set up a SparkContext that uses SURFsara's Hadoop cluster: [YARN resourcemanager](http://head05.hathi.surfsara.nl:8088/cluster) (note you will need to be authenticated via kerberos on your machine to visit the resourcemanager link) for executors.\n",
    "\n",
    "First initialize kerberos via a Jupyter terminal. \n",
    "In the terminal execute: <BR>\n",
    "<i>kinit -k -t data/robertop.keytab robertop@CUA.SURFSARA.NL</i><BR>\n",
    "Print your credentials:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticket cache: FILE:/tmp/krb5cc_1000\r\n",
      "Default principal: robertop@CUA.SURFSARA.NL\r\n",
      "\r\n",
      "Valid starting       Expires              Service principal\r\n",
      "04/26/2016 06:57:12  04/27/2016 06:57:12  krbtgt/CUA.SURFSARA.NL@CUA.SURFSARA.NL\r\n",
      "\trenew until 04/26/2016 06:57:12\r\n"
     ]
    }
   ],
   "source": [
    "! klist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\r\n",
      "drwx------   - robertop hdfs          0 2016-04-28 06:47 .Trash\r\n",
      "drwxr-xr-x   - robertop hdfs          0 2016-04-28 09:35 .sparkStaging\r\n",
      "drwx------   - robertop hdfs          0 2016-04-06 15:54 .staging\r\n",
      "drwxr-xr-x   - robertop hdfs          0 2016-04-27 13:07 mattia\r\n",
      "drwxr-xr-x   - robertop hdfs          0 2016-04-13 10:00 recsys2016Competition\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls \n",
    "execfile('../spark-scripts/bullet.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that we can browse HDFS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next initialize Spark. Note that the code below starts a job on the Hadoop cluster that will remain running while the notebook is active. Please close and halt the notebook when you are done. Starting the SparkContext can take a little longer. You can check the YARN resourcemanager to see the current status/usage of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/local/bin/python2.7'\n",
    "\n",
    "HDFS_PATH = \"hdfs://hathi-surfsara\"\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "sconf = SparkConf()\n",
    "\n",
    "# Master is now yarn-client. The YARN and hadoop config is read from the environment\n",
    "sconf.setMaster(\"yarn-client\")\n",
    "\n",
    "# You can control many Spark settings via the SparkConf. This determines the amount of executors on the cluster:\n",
    "sconf.set(\"spark.executor.instances\", \"200\")\n",
    "#sconf.set(\"spark.executor.memory\", \"20g\")\n",
    "\n",
    "# UFW (firewall) is active on the VM. We explicitly opened these ports and Spark should not bind to random ports:\n",
    "sconf.set(\"spark.driver.port\", 51800)\n",
    "sconf.set(\"spark.fileserver.port\", 51801)\n",
    "sconf.set(\"spark.broadcast.port\", 51802)\n",
    "sconf.set(\"spark.replClassServer.port\", 51803)\n",
    "sconf.set(\"spark.blockManager.port\", 51804)\n",
    "sconf.set(\"spark.authenticate\", True)\n",
    "sconf.set(\"spark.yarn.keytab\", \"/home/jovyan/work/data/robertop.keytab\")\n",
    "sconf.set(\"spark.yarn.access.namenodes\", HDFS_PATH + \":8020\")\n",
    "\n",
    "try:\n",
    "    sc = SparkContext(conf=sconf)\n",
    "    sqlCtx = SQLContext(sc) \n",
    "    sendNotificationToMattia(\"Spark Context\", \"Ready!\")\n",
    "except Exception, err:\n",
    "    sendNotificationToMattia(\"Fuck you!\", str(err)) \n",
    "    print str(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <hr style=\"clear: both\" />\n",
    "\n",
    "# Now you can run your code\n",
    "\n",
    "Pick a clustering algorithm (name of the file that provides a classify(x,y [,threshold]) function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "execfile('../spark-scripts/conventions.py')\n",
    "execfile('../spark-scripts/splitCluster.py')\n",
    "#execfile('../spark-scripts/utils.py')\n",
    "execfile('../spark-scripts/eval.py')\n",
    "execfile('../spark-scripts/implicitPlaylistAlgoFunctions.py')\n",
    "execfile('../spark-scripts/implicitPlaylistAlgoMain.py')\n",
    "\n",
    "CLUSTER_ALGO = 'jaccardBase'\n",
    "THRESHOLD = 0.9\n",
    "print str(THRESHOLD)[2:]\n",
    "execfile('../spark-scripts/' + CLUSTER_ALGO + '.py')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the conf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "\n",
    "BASE_PATH = HDFS_PATH + '/user/robertop/mattia'\n",
    "\n",
    "conf = {}\n",
    "\n",
    "conf['split'] = {}\n",
    "conf['split']['reclistSize'] = 100\n",
    "conf['split']['callParams'] = {}\n",
    "conf['split']['excludeAlreadyListenedTest'] = True\n",
    "conf['split']['name'] = 'test'\n",
    "conf['split']['split'] = conf['split']['name']\n",
    "conf['split']['minEventsPerUser'] = 5\n",
    "conf['split']['inputData'] = HDFS_PATH + '/user/robertop/mattia/clusterBase.split/SenzaRipetizioni_1'\n",
    "#conf['split']['inputData'] = 's3n://contentwise-research-poli/30musicdataset/newFormat/relations/sessions.idomaar'\n",
    "conf['split']['bucketName'] = BASE_PATH\n",
    "conf['split']['percUsTr'] = 0.05\n",
    "conf['split']['ts'] = int(0.75 * (1421745857 - 1390209860) + 1390209860) - 10000\n",
    "conf['split']['minEventPerSession'] = 5\n",
    "conf['split']['onlineTrainingLength'] = 5\n",
    "conf['split']['GTlength'] = 1\n",
    "conf['split']['minEventPerSessionTraining'] = 10\n",
    "conf['split']['minEventPerSessionTest'] = 11\n",
    "conf['split']['mode'] = 'session'\n",
    "conf['split']['forceSplitCreation'] = False\n",
    "conf['split'][\"prop\"] = {'reclistSize': conf['split']['reclistSize']}\n",
    "conf['split']['type'] = list\n",
    "conf['split']['out'] = HDFS_PATH + '/user/robertop/mattia/clusterBase.split/'\n",
    "conf['split']['location'] = '30Mdataset/relations/sessions'\n",
    "\n",
    "conf['evaluation'] = {}\n",
    "conf['evaluation']['metric'] = {}\n",
    "conf['evaluation']['metric']['type'] = 'recall'\n",
    "conf['evaluation']['metric']['prop'] = {}\n",
    "conf['evaluation']['metric']['prop']['N'] = [1,2,5,10,15,20,25,50,100]\n",
    "conf['evaluation']['name'] = 'recall@N'\n",
    "\n",
    "conf['general'] = {}\n",
    "conf['general']['clientname'] = \"clusterBase.split\"\n",
    "conf['general']['bucketName'] = 'head02.hathi.surfsara.nl/user/robertop/mattia'\n",
    "conf['general']['tracksPath'] = '30Mdataset/entities/tracks.idomaar.gz'\n",
    "\n",
    "conf['algo'] = {}\n",
    "conf['algo']['name'] = 'ClusterBase'\n",
    "conf['algo']['props'] = {}\n",
    "# ***** EXAMPLE OF CONFIGURATION *****#\n",
    "conf['algo']['props'][\"sessionJaccardShrinkage\"] = 5\n",
    "conf['algo']['props'][\"clusterSimilarityThreshold\"] = 0.1\n",
    "conf['algo']['props'][\"expDecayFactor\"] = 0.7\n",
    "# ****** END EXAMPLE ****************#\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Load the list of songs ad create clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracks: 4519105\n",
      "IDs: 4519105\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import string\n",
    "\n",
    "def my_replace_punct(x):\n",
    "    ret = \"\"\n",
    "    for i in x:\n",
    "        if i == '+':\n",
    "            ret += ' '\n",
    "        else:\n",
    "            ret += i\n",
    "    return ret\n",
    "\n",
    "tracksRDD = sc.textFile(BASE_PATH + '/30Mdataset/entities/tracks.idomaar.gz')\n",
    "tracksRDD = tracksRDD.repartition(200)\n",
    "tracksRDD = tracksRDD.map(lambda x: x.split('\\t')).map(lambda x: (x[1], json.loads(x[3])['name'].split('/') ) )\n",
    "tracksRDD = tracksRDD.map(lambda x: (x[0], \" \".join( (x[1][0], x[1][2]) ) )).distinct()\n",
    "tracksRDD = tracksRDD.map(lambda x : (x[0], my_replace_punct(x[1])))\n",
    "tracksRDD = tracksRDD.map(lambda x: (x[0], tokenize_song(x[1]), x[1]))\n",
    "\n",
    "#tracksRDD.saveAsPickleFile(BASE_PATH + '/30Mdataset/entities/tracksPickle')\n",
    "tracksIdsRDD = tracksRDD.map(lambda x: (int(x[0]), [x[0]]))\n",
    "\n",
    "try:\n",
    "    n_tracks = str(tracksRDD.count())\n",
    "    n_tracksID = str(tracksIdsRDD.count())\n",
    "    print \"Tracks: \" + n_tracks\n",
    "    print \"IDs: \" + n_tracksID\n",
    "    parts = str(tracksRDD.getNumPartitions())\n",
    "    sendNotificationToMattia(\"Tracks loaded. \" + parts + \" parts\", \"Tracks: \" + n_tracks + \"IDs: \" + n_tracksID)\n",
    "except Exception, err:\n",
    "    sendNotificationToMattia(\"Fuck you!\", str(err))    \n",
    "    print str(err)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove the 100 longest songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'74141', u'750321', u'3152622', u'2827538', u'3152620', u'3152621', u'2827545', u'3152619', u'3341180', u'3152623', u'3152618', u'2827540', u'2827541', u'2827552', u'2827542', u'141134', u'2827551', u'2827550', u'781026', u'2827544', u'2827553', u'1674589', u'3363685', u'2827546', u'2827549', u'2827543', u'2680576', u'3544157', u'2827547', u'2827548', u'3544156', u'2827539', u'2754946', u'928716', u'3152606', u'3375879', u'3152605', u'3152604', u'3152607', u'3152602', u'105580', u'3152603', u'3152601', u'3152608', u'1230495', u'410557', u'742648', u'2171839', u'2652403', u'2520106', u'3068573', u'3396589', u'1994217', u'3095948', u'2036942', u'2706385', u'1994226', u'3755457', u'3395838', u'1675513', u'1994216', u'922515', u'1994220', u'1886095', u'1600901', u'2955525', u'1994227', u'482609', u'1994221', u'1994214', u'1668615', u'1089324', u'2696994', u'2696995', u'828373', u'1089325', u'3206202', u'1994223', u'1807301', u'2702002', u'2696998', u'3341156', u'1994229', u'760742', u'751407', u'1886115', u'1994212', u'1994213', u'24041', u'1994228', u'3396574', u'1994218', u'1994219', u'1994222', u'2078372', u'2418300', u'1434551', u'3341167', u'3396988', u'1077883']\n"
     ]
    }
   ],
   "source": [
    "TOP_N = 100\n",
    "longSong = tracksRDD.map(lambda x: (x[0], len(x[1]))).takeOrdered(TOP_N, lambda x: -x[1])\n",
    "longSong = [i[0] for i in longSong]\n",
    "print longSong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4519005\n"
     ]
    }
   ],
   "source": [
    "REMOVE_LONG_TITLES = True\n",
    "if REMOVE_LONG_TITLES:\n",
    "    tracksRDD = tracksRDD.filter(lambda x: x[0] not in longSong)\n",
    "    print tracksRDD.count()\n",
    "    \n",
    "TESTING = False\n",
    "if TESTING:\n",
    "    tracksRDD = tracksRDD.take(50000)\n",
    "    tracksRDD = sc.parallelize(tracksRDD)\n",
    "    print tracksRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build RDD with word/bigram mapped to songs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'zz top', (u'4573230', u'ZZ Top She Loves My Automobile')), (u'top she', (u'4573230', u'ZZ Top She Loves My Automobile')), (u'she loves', (u'4573230', u'ZZ Top She Loves My Automobile'))]\n",
      "\n",
      "Total words with more than one song: 1443568\n",
      "Total number of songs present: 16339063\n",
      "[(u'zz top', 626238), (u'she loves', 626055), (u'loves my', 625843), (u'my automobile', 625805), (u'top she', 625804), (u'of the', 49035), (u'in the', 39210), (u'original mix', 37890), (u'and the', 22308), (u'on the', 16901), (u'radio edit', 15441), (u'to the', 14769), (u'album version', 13491), (u'in a', 11743), (u'live at', 11153), (u'digital remaster', 10886), (u'for the', 9801), (u'at the', 9649), (u'the world', 9117), (u'bonus track', 9028)]\n",
      "\n",
      "\n",
      "Song removed: 3410167\n"
     ]
    }
   ],
   "source": [
    "#Map to Bigram -> (id,name)\n",
    "def extract_bigrams(x):\n",
    "    result = []\n",
    "    tokens = x[1]\n",
    "    l = len(tokens)\n",
    "    for i in range(l-1):\n",
    "        bigram = tokens[i] + ' ' + tokens[i+1]\n",
    "        result.append((bigram, (x[0], x[2])))\n",
    "    return result\n",
    "\n",
    "#Map to word -> (id,name)\n",
    "def extract_words(x):\n",
    "    result = []\n",
    "    tokens = x[1]\n",
    "    for t in tokens:\n",
    "        result.append( (t, (x[0], x[2])) )\n",
    "    return result\n",
    " \n",
    "''' CHOOSE ONE OF THE ABOVE FUNCTIONS FOR THE FLAT MAP '''\n",
    "#Build an RDD with ('word' or 'bigram' -> (id, name))\n",
    "wordsRDD = tracksRDD.flatMap( extract_bigrams )\n",
    "print wordsRDD.take(3)\n",
    "\n",
    "#Group by 'word' and keep only the ones with more then 1 song\n",
    "wordsRDD = wordsRDD.groupByKey().filter(lambda x: len(x[1]) > 1)\n",
    "\n",
    "#Eliminate words in top 0.1% of words\n",
    "try:\n",
    "    total_words = wordsRDD.count()\n",
    "    print \"\\nTotal words with more than one song: \" + str(total_words)\n",
    "    \n",
    "    nwords = 20 #int(total_words*0.001)\n",
    "    top_words_count = wordsRDD.map(lambda x: (x[0], len(x[1])))\n",
    "    \n",
    "    total_number_of_songs = top_words_count.reduce(lambda x,y: (0, x[1] + y[1]))[1]\n",
    "    print \"Total number of songs present: \" + str(total_number_of_songs)\n",
    "    \n",
    "    top_words_count = top_words_count.takeOrdered(nwords, lambda x: -x[1])\n",
    "    \n",
    "    print str(top_words_count) + \"\\n\"\n",
    "    \n",
    "    song_removed = 0\n",
    "    top_words = []\n",
    "    for i in top_words_count:\n",
    "        song_removed += i[1]\n",
    "        top_words.append(i[0])\n",
    "    \n",
    "    print \"\\nSong removed: \" + str(song_removed)\n",
    "    \n",
    "    wordsRDD = wordsRDD.filter(lambda x: x[0] not in top_words)\n",
    "    \n",
    "    sendNotificationToMattia(\"Removed Words - Songs: removed \" + str(song_removed) + \" / \" + str(total_number_of_songs) , str(top_words_count))\n",
    "    \n",
    "except Exception, err:\n",
    "    sendNotificationToMattia(\"Fuck you!\", str(err))    \n",
    "    print str(err)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify songs inside the same list (word/bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692106\n"
     ]
    }
   ],
   "source": [
    "#Compute a cartesian product for each list of songs with a common word\n",
    "def filtered_cartesian(x):\n",
    "    equal_couples = set()\n",
    "    x_1 = list(x[1])\n",
    "    for i in range(len(x_1)):\n",
    "        a = x_1[i]\n",
    "        id_a = x_1[i][0]\n",
    "        name_a = x_1[i][1]\n",
    "        \n",
    "        for j in range(i):\n",
    "            b = x_1[j]\n",
    "            id_b = x_1[j][0]\n",
    "            name_b = x_1[j][1]\n",
    "            if id_a != id_b:\n",
    "                if classify(name_a, name_b, THRESHOLD):\n",
    "                    equal_couples.add( (int(id_a), (id_a, id_b)) )\n",
    "                    equal_couples.add( (int(id_b), (id_a, id_b)) )\n",
    "                    \n",
    "    return list(equal_couples)\n",
    "\n",
    "coupleRDD = wordsRDD.flatMap(filtered_cartesian)\n",
    "try:\n",
    "    n_couples = coupleRDD.count()\n",
    "    print n_couples\n",
    "    sendNotificationToMattia(\"Words!\", str(n_couples) + \" Couples found.\")\n",
    "except Exception, err:\n",
    "    sendNotificationToMattia(\"Fuck you!\", str(err))\n",
    "    print str(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group by song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234583\n",
      "[(2201600, [u'2201600', u'2201601']), (1894400, [u'1894399', u'1894400']), (3883400, [u'3883400', u'2658423']), (1832800, [u'1832801', u'1832800']), (1885200, [u'1885209', u'1885201', u'1885200', u'1885243']), (2989600, [u'2989599', u'2989600']), (1441800, [u'1441801', u'1441800', u'1441803', u'1441802']), (672000, (u'672000', u'672001')), (3746600, [u'3746601', u'3746600']), (3037200, [u'3037200', u'3037183', u'3037317'])]\n"
     ]
    }
   ],
   "source": [
    "#Group by key (song). Each song has now one cluster\n",
    "def merge_couples(x, y):\n",
    "    return list(set(x) | set(y))\n",
    "\n",
    "try:\n",
    "    songClusterRDD = coupleRDD.reduceByKey(merge_couples)\n",
    "    print songClusterRDD.count()\n",
    "    print songClusterRDD.take(10)\n",
    "    sendNotificationToMattia(\"Song to Cluster!\", str(songClusterRDD.take(10)))\n",
    "except Exception, err:\n",
    "    sendNotificationToMattia(\"Fuck you!\", str(err))\n",
    "    print str(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify with all the Songs (not clustered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4519105\n",
      "[(2457600, [u'2457599', u'2457600']), (4505600, [u'4505600']), (1796000, [u'1796000']), (898000, [u'898000']), (0, [u'0']), (4846800, [u'4846800']), (3050800, [u'3050800']), (2883600, [u'2883600']), (2152800, [u'2152800']), (1254800, [u'1254801', u'1254800'])]\n"
     ]
    }
   ],
   "source": [
    "#In this way we obtain a complete RDD with song -> group of songs\n",
    "def reduce_to_biggest(x, y):\n",
    "    bigger = x if len(x) > len(y) else y\n",
    "    result = sorted(bigger)\n",
    "    return result\n",
    " \n",
    "unionJoinRDD = tracksIdsRDD.leftOuterJoin(songClusterRDD)\n",
    "unionRDD = unionJoinRDD.map(lambda x: (x[0], x[1][0]) if x[1][1] == None else (x[0], x[1][1]))\n",
    "\n",
    "try:\n",
    "    tot_n = unionRDD.count()\n",
    "    print tot_n\n",
    "    print unionRDD.take(10)\n",
    "    sendNotificationToMattia(str(tot_n) + \" Tracks unified with all tracks ids\", \"Inverting and unifying clusters...\")\n",
    "except Exception, err:\n",
    "    sendNotificationToMattia(\"Fuck you!\", str(err))\n",
    "    print str(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build indexing for clusters (new IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4399492\n"
     ]
    }
   ],
   "source": [
    "#Flip the mapping as cluster->song\n",
    "clusterSongsRDD = unionRDD.map(lambda x: (' '.join(x[1]), x[0])).groupByKey().mapValues(list)\n",
    "clusterSongsRDD = clusterSongsRDD.zipWithIndex().map(lambda x: (x[1], x[0][1]))\n",
    "\n",
    "try:\n",
    "    tot_n = clusterSongsRDD.count()\n",
    "    print tot_n\n",
    "    sendNotificationToMattia(str(tot_n) + \" Unique clusters found!\", \"Writing...\")\n",
    "except Exception, err:\n",
    "    sendNotificationToMattia(\"Fuck you!\", str(err))\n",
    "    print str(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Save clustering\n",
    "THRESHOLD_STR = str(THRESHOLD)[2:]\n",
    "try:\n",
    "    clusterSongsRDD.saveAsPickleFile(BASE_PATH + '/clusters/' + CLUSTER_ALGO + THRESHOLD_STR)\n",
    "    sendNotificationToMattia(\"File Written!\", BASE_PATH + '/clusters/' + CLUSTER_ALGO + THRESHOLD_STR)\n",
    "except Exception, err:\n",
    "    sendNotificationToMattia(\"Fuck you!\", str(err))\n",
    "    print str(err)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
